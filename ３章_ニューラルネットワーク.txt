３章　ニューラルネットワーク
■目的
  ニューラルネットワークが識別を行う際の処理を学ぶ
■理由
  パーセプトロンでは、重みづけを人の手で行う必要があったが、
  ニューラルネットワークでは適切な重みパラメータを自動で学習することができるから

■アジェンダ
  1.パーセプトロンからニューラルネットワークへ
  2.活性化関数
  3.多次元配列の計算
  4.３層ニューラルネットワークの実装
  5.出力層の設計
  6.手書き数字認識

■内容
  1. パーセプトロンからニューラルネットワークへ
    パーセプトロン（単純パーセプトロン）・・・単層で、ステップ関数を活性化関数に用いる
    ニューラルネットワーク（多層パーセプトロン）・・・多層で、シグモイド関数などの滑らかな活性化関数を用いる
  2. 活性化関数
    前提条件：「非線形関数」＝「定数倍じゃない」
    理由：多層にする意味がないから

    ⑴シグモイド関数
      ・よく使われるやつ
      h(x) = 1/{1 + exp(-x)}
      　→滑らかなステップ関数

    ⑵ReLU関数
      最近よく使われるやつ
      h(x) = x (x > 0)
             0 (x <= 0)
      →めちゃくちゃシンプル
      →max(0,x)

  3. 多次元配列の計算
    (1)Numpyの多次元配列を使って、効率的に実装する
      //3*2の配列：A
      A = np.array([[1,2],[3,4],[5,6]])
      //2*3の配列：B
      B = no.array([[1,2,3],[4,5,6]])
      //3*3の配列：C＝A・B
      c = np.dot(A,B)

  4. ３層ニューラルネットワークの実装
    第０層（入力）：X([x1,x2])
    バイアス：B1
    重み：W１([[w11,w21,w31],[w12,w22,w32]])
    第1層(隠れ層)：A1 = np.dot(X,W1) + B1 = [a1,a2,a3]
    活性化：Z1 = sigmoid(A1)

    バイアス：B2
    重み：W２([[w11,w21],[w12,w22],[W31,W32]])
    第2層(隠れ層)：A2 = np.dot(Z1, W2) + B2 = [a1,a2]
    活性化：Z2 = sigmoid(A2)

    バイアス：B3
    重み：W３([w11,w21],[w12,w22],[w13,w23])
    第3層（最終層）：A3 = np.dot(Z2,W3) + B3
    出力：Y([y1,y2]) = identify_function(A3)

    #ソースコード：

  5.　出力層の設計
    ・回帰問題→恒等関数
    ・分類問題→ソフトマックス関数
      を用いるのが一般的
      #ソースコード：ソフトマックス関数

  6.　手書き数字認識
    MNISTデータセットの手書き文字を使って、
    手書き数字認識の、推論処理（順方向伝播）を学ぶ
    ※学習はまだあと、学習が終わった前提で、学習後のニューラルネットを、
    "sample_weight.pkl"から呼び出している

    ・バッチ処理
    →　一個のデータをデータ数分呼びだすよりも、
    　ひとまとまり（バッチ）にして、大きな配列を一度に計算するほうが、
      計算量が少なくて済む
